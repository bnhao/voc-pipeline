import sqlite3
import pandas as pd
import re
import os

# --- CONFIGURATION ---
# We use the large dataset generated by Phase 0
CSV_INPUT_PATH = 'data/atb_large_dataset.csv'
DB_PATH = 'data/atb_mock_db.sqlite'
CSV_OUTPUT_PATH = 'data/clean_feedback.csv'

# Ensure data directory exists
os.makedirs('data', exist_ok=True)

def create_mock_database():
    if not os.path.exists(CSV_INPUT_PATH):
        print(f"Warning: Large dataset not found at {CSV_INPUT_PATH}. Please run 'src/00_generate_data.py' first.")
        return

    print(f"üì• Loading {CSV_INPUT_PATH} into SQL Database...")
    
    # Load CSV
    df_csv = pd.read_csv(CSV_INPUT_PATH)
    
    # Create SQL Connection
    conn = sqlite3.connect(DB_PATH)
    
    # Save to SQL (Replacing any old table)
    df_csv.to_sql('ClientFeedback', conn, if_exists='replace', index=False)
    
    conn.close()
    print(f"‚úÖ SQL Database updated with {len(df_csv)} records.")

def clean_text(text):
    """
    Cleaning pipeline:
    1. Handle Nulls/NaNs (Critical for real data)
    2. Remove HTML tags
    3. Remove special characters
    4. Lowercase & Trim
    """
    # Check if text is a string (handles NaN, float, None)
    if not isinstance(text, str):
        return ""
    
    # Remove HTML tags
    text = re.sub(r'<.*?$>', '', text)

    # Remove special characters but keep spaces and basic punctuation
    text = re.sub(r'[^a-zA-Z0-9\s.]', '', text)
    
    # Convert to lowercase and strip whitespace
    return text.lower().strip()

def run_pipeline():
    print("--- üöÄ Starting Phase 1: Data Ingestion & Cleaning ---")
    
    # Step A: Load the Large Data into SQL
    create_mock_database()
    
    # Step B: Connect to DB (SQL Extraction Skill)
    conn = sqlite3.connect(DB_PATH)
    try:
        # We read from the table we just created
        df = pd.read_sql("SELECT * FROM ClientFeedback", conn)
        print(f"Extracted {len(df)} raw records from SQL.")
    except Exception as e:
        print(f"‚ùå SQL Error: {e}")
        conn.close()
        return
    conn.close()
    
    # Step C: Data Cleaning (Pandas Skill)
    print("üßπ Cleaning data...")
    
    if 'client_comment' in df.columns:
        df['clean_comment'] = df['client_comment'].apply(clean_text)
    elif 'comment' in df.columns:
        df['clean_comment'] = df['comment'].apply(clean_text)
    else:
        raise KeyError(f"Column 'client_comment' not found. Available columns: {df.columns.tolist()}")
    
    # Filter out empty rows (Quality Data Foundation)
    initial_count = len(df)
    df = df[df['clean_comment'] != ""]
    final_count = len(df)
    
    print(f"Removed {initial_count - final_count} invalid/empty rows.")
    
    # Step D: Save to CSV (Pipeline persistence)
    df.to_csv(CSV_OUTPUT_PATH, index=False)
    print(f"Success! Clean data saved to: {CSV_OUTPUT_PATH}")
    
    # Preview
    print("\n--- Preview of Clean Data ---")
    print(df[['client_comment', 'clean_comment']].head())

if __name__ == "__main__":
    run_pipeline()